{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.pipeline.core import DownloadMethod\n",
    "import spacy\n",
    "import benepar\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "from nltk import Tree\n",
    "from nltk.parse.stanford import StanfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_neural_models():\n",
    "    stanza.download('en') # Downloads English models for neural pipeline\n",
    "    benepar.download('benepar_en3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once\n",
    "# download_neural_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stanfordnlp.github.io/stanza/getting_started.html#downloading-models-for-offline-usage\n",
    "# download_method = DownloadMethod.REUSE_RESOURCES\n",
    "# logging_level = \"ERROR\"\n",
    "nlp1 = stanza.Pipeline('en',download_method=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Barrack Obama was born in Hawaii. He was elected President in 2008.\"\n",
    "text2 = \"He does not like Chomsky's work. He loves Halliday's SFL.\"\n",
    "text3 = \"Time flies like an arrow.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp1(text1)\n",
    "for sent in doc1.sentences:\n",
    "    # sent.print_dependencies()\n",
    "    # print(sent.constituency)\n",
    "    parse_tree_string = str(sent.constituency)\n",
    "    print(parse_tree_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "# benepar_model = benepar.BeneparComponent('benepar_en3') # Choose appropiate model\n",
    "# nlp2.add_pipe(benepar_model,name=\"benepar\") \n",
    "# nlp2.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp2(text1)\n",
    "for sent in doc2.sents:\n",
    "    print(sent)\n",
    "\n",
    "displacy.render(doc2,style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp2(text3)\n",
    "print(list(doc3.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy\n",
    "\n",
    "def tok_format(tok):\n",
    "    return \"_\".join([tok.orth_, tok.tag_])\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return tok_format(node)\n",
    "\n",
    "\n",
    "# [to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]\n",
    "for sent in doc2.sents :\n",
    "   nlk_tree = to_nltk_tree(sent.root)\n",
    "   nlk_tree.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Tokenize the sentence\n",
    "# tokens = nltk.word_tokenize(text3)\n",
    "\n",
    "# # Parse the sentence using the NLTK Recursive Descent Parser\n",
    "# parser = nltk.RecursiveDescentParser(nltk.CFG.fromstring(\"\"\"\n",
    "#     S -> NP VP\n",
    "#     VP -> V NP | V NP PP\n",
    "#     PP -> P NP\n",
    "#     V -> \"flies\" | \"like\"\n",
    "#     NP -> \"Time\" | Det N | Det N PP\n",
    "#     Det -> \"an\"\n",
    "#     N -> \"arrow\" | \"Time\" | \"flies\"\n",
    "#     P -> \"like\"\n",
    "#     \"\"\"))\n",
    "\n",
    "# num_parse_trees = len(list(parser.parse(tokens)))\n",
    "# print(f\"The number of parse trees for the sentence '{sentence}' is: {num_parse_trees}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set the path to the Stanford Parser JAR file\n",
    "# stanford_parser_jar = 'assets/stanford-parser.jar'\n",
    "stanford_parser_jar = 'assets/stanford-parser-full-2020-11-17/stanford-parser.jar'\n",
    "# Set the path to the Stanford Parser models (English PCFG)\n",
    "# stanford_models_jar = 'assets/stanford-parser-4.2.0-models.jar'  # Adjust the version as needed\n",
    "stanford_models_jar = 'assets/stanford-parser-4.2.0-models.jar'\n",
    "\n",
    "# Create a Stanford Parser : Deprecated\n",
    "# parser = StanfordParser(\n",
    "#     path_to_jar=stanford_parser_jar,\n",
    "#     path_to_models_jar=stanford_models_jar\n",
    "# )\n",
    "\n",
    "# parser = nltk.parse.corenlp.CoreNLPParser()\n",
    "# Parse the sentence and print all parses\n",
    "# for tree in parser.raw_parse(sentence):\n",
    "#     print(tree)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
