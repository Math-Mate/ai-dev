{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.pipeline.core import DownloadMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_neural_models():\n",
    "    stanza.download('en') # Downloads English models for neural pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once\n",
    "# download_neural_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text :  Barrack Obama was born in Hawaii. He was elected President in 2008.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 9.69 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 2.51 GiB is allocated by PyTorch, and 24.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/andy/Code/AIprojects/nlp-1/test.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andy/Code/AIprojects/nlp-1/test.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m download_method \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andy/Code/AIprojects/nlp-1/test.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m logging_level \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mERROR\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andy/Code/AIprojects/nlp-1/test.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m nlp \u001b[39m=\u001b[39m stanza\u001b[39m.\u001b[39;49mPipeline(\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m,download_method\u001b[39m=\u001b[39;49mdownload_method,logging_level\u001b[39m=\u001b[39;49mlogging_level)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andy/Code/AIprojects/nlp-1/test.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/andy/Code/AIprojects/nlp-1/test.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m doc\u001b[39m.\u001b[39msentences[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mprint_dependencies()\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/stanza/pipeline/core.py:304\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, resources_filepath, proxies, foundation_cache, device, allow_unknown_language, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m logger\u001b[39m.\u001b[39mdebug(curr_processor_config)\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[39m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name] \u001b[39m=\u001b[39m NAME_TO_PROCESSOR_CLASS[processor_name](config\u001b[39m=\u001b[39;49mcurr_processor_config,\n\u001b[1;32m    305\u001b[0m                                                                               pipeline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    306\u001b[0m                                                                               device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    307\u001b[0m \u001b[39mexcept\u001b[39;00m ProcessorRequirementsException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    308\u001b[0m     \u001b[39m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     pipeline_reqs_exceptions\u001b[39m.\u001b[39mappend(e)\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/stanza/pipeline/depparse_processor.py:30\u001b[0m, in \u001b[0;36mDepparseProcessor.__init__\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, pipeline, device):\n\u001b[1;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pretagged \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config, pipeline, device)\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/stanza/pipeline/processor.py:193\u001b[0m, in \u001b[0;36mUDProcessor.__init__\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_variant\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_up_model(config, pipeline, device)\n\u001b[1;32m    195\u001b[0m \u001b[39m# build the final config for the processor\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_up_final_config(config)\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/stanza/pipeline/depparse_processor.py:43\u001b[0m, in \u001b[0;36mDepparseProcessor._set_up_model\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pretrain \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mfoundation_cache\u001b[39m.\u001b[39mload_pretrain(config[\u001b[39m'\u001b[39m\u001b[39mpretrain_path\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mpretrain_path\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m config \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     41\u001b[0m args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mcharlm_forward_file\u001b[39m\u001b[39m'\u001b[39m: config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mforward_charlm_path\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m     42\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcharlm_backward_file\u001b[39m\u001b[39m'\u001b[39m: config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbackward_charlm_path\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)}\n\u001b[0;32m---> 43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer \u001b[39m=\u001b[39m Trainer(args\u001b[39m=\u001b[39;49margs, pretrain\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrain, model_file\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mmodel_path\u001b[39;49m\u001b[39m'\u001b[39;49m], device\u001b[39m=\u001b[39;49mdevice, foundation_cache\u001b[39m=\u001b[39;49mpipeline\u001b[39m.\u001b[39;49mfoundation_cache)\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/stanza/models/depparse/trainer.py:40\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, args, vocab, pretrain, model_file, device, foundation_cache)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39m=\u001b[39m vocab\n\u001b[1;32m     39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m Parser(args, vocab, emb_matrix\u001b[39m=\u001b[39mpretrain\u001b[39m.\u001b[39memb \u001b[39mif\u001b[39;00m pretrain \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> 40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_optimizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39moptim\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m], betas\u001b[39m=\u001b[39m(\u001b[39m0.9\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mbeta2\u001b[39m\u001b[39m'\u001b[39m]), eps\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m, bert_learning_rate\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbert_learning_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.0\u001b[39m))\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Code/AIprojects/nlp-1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 9.69 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 2.51 GiB is allocated by PyTorch, and 24.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Barrack Obama was born in Hawaii.\"\n",
    "text += \" He was elected President in 2008.\"\n",
    "print(\"Text : \",text)\n",
    "\n",
    "# https://stanfordnlp.github.io/stanza/getting_started.html#downloading-models-for-offline-usage\n",
    "# download_method = DownloadMethod.REUSE_RESOURCES\n",
    "download_method = None\n",
    "logging_level = \"ERROR\"\n",
    "nlp = stanza.Pipeline('en',download_method=download_method,logging_level=logging_level)\n",
    "doc = nlp(text)\n",
    "\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
